{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像分类网络之ConvNeXt\n",
    "\n",
    "## 模型简介\n",
    "\n",
    "ConvNeXt网络由Facebook AI研究所和UC Berkeley大学共同提出，它是一个面向2020s年代的卷积神经网络模型，并在论文[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)中首次对其进行描述。\n",
    "\n",
    "ConvNeXt网络并没有在整体的网络框架和搭建思路上做出重大的创新，它主要是按照Transformer网络的一些思想对现有的经典ResNet网络做了一些改进，该网络在多个分类任务和识别任务中均超越了Swin-T模型达到最佳的性能表现。其中在ImageNet-1K的分类任务中，ConvNext网络与其他经典网络相比，性能如下图所示。\n",
    "\n",
    "<div align=center><img src=\"./images/conv1.png\"></div>\n",
    "\n",
    "其中，图的左半部分可发现与ReNet、Swin-T等经典模型相比，ConvNext对分辨率为224x224的ImageNet-1K具有更好的分类精度。相同的，在图的右半部分中与ViT、Swin-T等经典模型相比，ConvNext对分辨率为384x384的ImageNet-22K具有更高的精度。不论是ImageNet-1K还是ImageNet-22K，ConvNext网络将2010年代的网络精度进行了提升，因此论文将所提出的ConvNext称为“2020年代的卷积网络”。\n",
    "\n",
    "### 网络特点\n",
    "\n",
    "下图概括了ConvNext所有的优化点，同时图中还给出了每一个优化点对网络精度以及GFLOPs的影响。它从ResNet-50/ResNet-200出发，依次从宏观设计（Macro design）、深度可分离卷积（ResNeXt）、逆瓶颈层（Inverted bottleneck）、大卷积核（Large kernel size）以及细节设计（Various layer-wise Micro designs）这五个方面依次借鉴了Swin Transformer的思想，然后在ImageNet-1K上进行了训练和评估，由下图可发现，在相同的GFLOPs下，相较于Swin TransFormer，ConvNext-T/B精度提升了0.7。\n",
    "\n",
    "<div align=center><img src=\"./images/conv2.png\"></div>\n",
    "\n",
    "接下来依次介绍改动的五个部分，但在详细介绍每个部分之前，先介绍一下训练方法上的改进。\n",
    "\n",
    "#### 训练技巧\n",
    "\n",
    "论文采用与Swin Transformer相似的训练方法，大致从以下四个方法进行改进：\n",
    "\n",
    "（1）epoch从ResNet的90增加到300；\n",
    "\n",
    "（2）优化器从SGD改用AdamW优化器；\n",
    "\n",
    "（3）在数据增强方面引进Mixup、Cutmix、RandAugment和Random Erasing等；\n",
    "\n",
    "（4）增加正则化策略，例如使用随机深度、标签平滑、EMA等。\n",
    "\n",
    "更加具体的预训练和微调技巧的超参数如下图。\n",
    "\n",
    "<div align=center><img src=\"./images/train.jpg\"></div>\n",
    "\n",
    "采用这些训练技巧后，ResNet-50的性能提升了2.7%，从76.1%提升到78.8%。该结果证明，传统卷积与Transformer之间的性能差异也有一部分来自于训练的方法。\n",
    "\n",
    "接下来对模型本身五大优化点进行详细的介绍。\n",
    "\n",
    "#### Macro design\n",
    "\n",
    "宏观设计部分主要有两项改动：\n",
    "\n",
    "（1）stage比例：\n",
    "\n",
    "ResNet和Swin-T网络均有四个stage阶段，其中Swin-T各个阶段堆叠Block块的比例为1:1:3:1，Swin-L 堆叠的比例为 1:1:9:3，由此可以发现 Transformer 网络的第三层的堆叠数量较多，因此ConvNeXt网络依照这个比例将ResNet各阶段的堆叠次数从 (3, 4, 6, 3)调整为(3, 3, 9, 3) ，其比例也保持在 1:1:3:1,改动如下图所示。\n",
    "\n",
    "<div align=center><img src=\"./images/macro.png\"></div>\n",
    "\n",
    "这项改动使模型精度提高了0.6%，到达79.4%。\n",
    "\n",
    "（2）Patchify Stem:\n",
    "\n",
    "Swin-T 网络中的 stem 层为一个卷积核大小为4，步距为4的卷积层，而经典的 ResNet50的stem层是由一个卷积核大小为7，步距为2的卷积层加一个核大小为3，步距为2的最大池化层构成的。因此,ConvNeXt网络将stem层换成了与Swin-T网络相同的卷积核大小为4，步距为4的卷积层。这项改动给模型精度再度带来0.1%的提升，精度达到79.5%。\n",
    "\n",
    "#### ResNeXt-ify\n",
    "\n",
    "这一部分中，尝试使用ResNeXt的核心思想--分组卷积，其中为弥补模型容量上的损失增加了网络宽度。同时ConvNext直接让分组数与输入通道数相等，设为96。这样每个卷积核处理一个通道，只在空间维度上做信息混合，获得与自注意力机制类似的效果。这项改动使网络性能再提高1%，达到80.5%。\n",
    "\n",
    "#### Inverted bottleneck\n",
    "\n",
    "在Transformer网络中的MLP模块及MobileNet V2中的Inverted Bottleneck模块，都是采用“两头细，中间粗”的反瓶颈结构。因此，ConvNeXt 网络也参照设计了一个类似的 Inverted bottleneck 结构，如下图所示，该过程为从（a）到（b）。\n",
    "\n",
    "<div align=center><img src=\"./images/invert.jpg\"></div>\n",
    "\n",
    "在做完这样的反转之后，虽然depthwise卷积层的FLOPs有所增加，但下采样残差块作用下，整个网络的FLOPs反而被减少，模型精度也提高了0.1%，达到了80.6%。\n",
    "\n",
    "#### Large kernel size\n",
    "\n",
    "在经典的CNN网络中我们一般习惯于使用 3×3 的卷积核，Swin Transformer引入了类似卷积核的局部窗口机制，但大小至少有7x7。而 ConvNeXt测试了各种不同尺寸的卷积核，在测试的过程中发现，反转瓶颈层之后放大了卷积层的维度，直接增大卷积核会让参数量显著增加。所以在这之前，还要再做一步操作，在反转瓶颈层的基础上把depthwise卷积层提前，如下图（b）到（c）。\n",
    "\n",
    "<div align=center><img src=\"./images/Large.jpg\"></div>\n",
    "\n",
    "这项改动暂时将模型精度下降到了79.9%。之后对卷积核大小的试验从3x3到11x11都有尝试，在7x7时模型精度重回80.6%。再往上增加效果则不明显，在ResNet-200上同样如此，最后卷积核大小就定在7x7。\n",
    "\n",
    "#### Various layer-wise Micro designs\n",
    "\n",
    "该部分主要将重点放在了激活函数和归一化上，主要进行以下五部分的微观改动。\n",
    "\n",
    "（1）传统的 CNN 网络中通常使用 RELU 作为网络的激活函数，而目前 Transformer 类型的网络主流上采用 GELU 激活函数，因此 ConvNeXt 网络将 RELU 替换为更常用的 GELU 激活函数。\n",
    "\n",
    "（2）Swin-T 网络的每一个 Swin Transformer Block 中均只含有一个激活函数，因此受 Swin-T 的启发，ConvNeXt 网络减少了激活函数的使用，每个块只使用一个激活函数，部署在在第二层之后。\n",
    "\n",
    "（3）与激活函数类似，ConvNeXt 网络也减少了正则化函数的使用，每个块只使用一个正则化函数，部署在第一层之后。\n",
    "\n",
    "（4）ConvNeXt 不仅减少了正则化函数的使用，还将正则化函数由 BN 替换成 LN。\n",
    "\n",
    "（5）参考 Swin-T 网络中的 Patch Merging 模块，ConvNeXt 网络单独设计了一个下采样层对特征进行单独的下采样操作。\n",
    "\n",
    "将以上所有改动汇总起来，ConvNext单个块的结构如下图所示。最终精度达到82.0%，优于Swin-T的81.3%。\n",
    "\n",
    "<div align=center><img src=\"./images/Microdesign.png\"></div>\n",
    "\n",
    "> 本教程将使用ImageNet数据集对ConvNeXt网络进行训练，并对测试结果进行可视化展示。为了节省运行时间，建议用户使用Ascend来运行本实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据\n",
    "\n",
    "开始实验之前，请确保本地已经安装了Python环境并安装了MindSpore Vision套件1.7.0版本。\n",
    "\n",
    "### 数据准备\n",
    "\n",
    "在本教程中，我们将使用[ImageNet数据集](https://image-net.org/)，该数据集总共1000个类，每张都是224*224的彩色图像。其中训练集共1,281，167张图像，测试集共50,000张图像。\n",
    "本案例应用的数据集是ImageNet中筛选出来的子集，运行第一段代码时会自动下载并解压。请确保你的数据集路径如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Text\n",
    ".ImageNet/\n",
    "    ├── ILSVRC2012_devkit_t12.tar.gz\n",
    "    ├── train/\n",
    "    ├── infer/\n",
    "    └── val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载及处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行数据处理之前，先导入本案例所需的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter is finished.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse parameters.\n",
    "\n",
    "    Returns:\n",
    "        parsed parameters.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='config')\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"seed for initializing training. \")\n",
    "    parser.add_argument(\"--device_target\", default=\"Ascend\", choices=[\"GPU\", \"Ascend\"], type=str)\n",
    "    parser.add_argument(\"--device_id\", default=0, type=int, help=\"device id\")\n",
    "    parser.add_argument(\"-a\", \"--arch\", metavar=\"ARCH\", default=\"convnext_tiny\", help=\"model architecture\")\n",
    "    parser.add_argument(\"--in_chans\", default=3, type=int)\n",
    "    parser.add_argument(\"--num_classes\", default=1000, type=int)\n",
    "    parser.add_argument(\"--drop_path_rate\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--amp_level\", default=\"O1\", choices=[\"O0\", \"O1\", \"O2\", \"O3\"], help=\"AMP Level\")\n",
    "    parser.add_argument(\"--label_smoothing\", type=float, help=\"label smoothing to use, default 0.1\", default=0.1)\n",
    "    parser.add_argument(\"--mix_up\", default=0.8, type=float, help=\"mix up\")\n",
    "    parser.add_argument(\"--cutmix\", default=1.0, type=float, help=\"cutmix\")\n",
    "    parser.add_argument(\"--run_modelarts\", type=ast.literal_eval, default=False, help=\"whether run on modelarts\")\n",
    "    parser.add_argument(\"--pretrained\", dest=\"pretrained\", default=\"/home/ma-user/work/check/src/train_parallel0/convnext_tiny0-300_533.ckpt\", type=str, help=\"use pre-trained model\")\n",
    "    parser.add_argument(\"--set\", help=\"name of dataset\", type=str, default=\"ImageNet\")\n",
    "    parser.add_argument('--data_url', default=\"/home/ma-user/work/imagenet2012\", help='location of data.')\n",
    "    parser.add_argument(\"--image_size\", default=224, help=\"image Size.\", type=int)\n",
    "    parser.add_argument('--interpolation', type=str, default=\"bicubic\")\n",
    "    parser.add_argument('--auto_augment', type=str, default=\"rand-m9-mstd0.5-inc1\")\n",
    "    parser.add_argument(\"-j\", \"--num_parallel_workers\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"number of data loading workers (default: 1)\")\n",
    "    parser.add_argument(\"--batch_size\", default=300, type=int, metavar=\"N\",\n",
    "                        help=\"mini-batch size (default: 256), this is the total \"\n",
    "                             \"batch size of all Devices on the current node when \"\n",
    "                             \"using Data Parallel or Distributed Data Parallel\")\n",
    "    parser.add_argument(\"--re_prob\", default=0.0, type=float, help=\"re prob\")\n",
    "    parser.add_argument('--re_mode', type=str, default=\"pixel\")\n",
    "    parser.add_argument(\"--re_count\", default=1, type=int, help=\"re count\")\n",
    "    parser.add_argument(\"--mixup_prob\", default=1., type=float, help=\"mixup prob\")\n",
    "    parser.add_argument(\"--switch_prob\", default=0.5, type=float, help=\"switch prob\")\n",
    "    parser.add_argument(\"--mixup_mode\", default='batch', type=str, help=\"mixup_mode\")\n",
    "    parser.add_argument(\"--optimizer\", help=\"Which optimizer to use\", default=\"adamw\")\n",
    "    parser.add_argument(\"--lr_scheduler\", default=\"cosine_lr\", help=\"schedule for the learning rate.\")\n",
    "    parser.add_argument(\"--warmup_length\", default=20, type=int, help=\"number of warmup iterations\")\n",
    "    parser.add_argument(\"--warmup_lr\", default=0.00000007, type=float, help=\"warm up learning rate\")\n",
    "    parser.add_argument(\"--base_lr\", default=0.004, type=float, help=\"base learning rate\")\n",
    "    parser.add_argument(\"--epochs\", default=300, type=int, metavar=\"N\", help=\"number of total epochs to run\")\n",
    "    parser.add_argument(\"--min_lr\", default=0.0000006, type=float, help=\"min learning rate\")\n",
    "    parser.add_argument(\"--start_epoch\", default=0, type=int, metavar=\"N\",\n",
    "                        help=\"manual epoch number (useful on restarts)\")\n",
    "    parser.add_argument(\"--accumulation_step\", default=1, type=int, help=\"accumulation step\")\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, metavar=\"M\", help=\"momentum\")\n",
    "    parser.add_argument(\"--beta\", default=[0.9, 0.999], type=lambda x: [float(a) for a in x.split(\",\")],\n",
    "                        help=\"beta for optimizer\")\n",
    "    parser.add_argument(\"--eps\", default=1e-8, type=float)\n",
    "    parser.add_argument(\"--wd\", \"--weight_decay\", default=0.05, type=float, metavar=\"W\",\n",
    "                        help=\"weight decay (default: 0.05)\", dest=\"weight_decay\")\n",
    "    parser.add_argument(\"--is_dynamic_loss_scale\", default=True, type=bool, help=\"is_dynamic_loss_scale \")\n",
    "    parser.add_argument(\"--loss_scale\", default=1024, type=int, help=\"loss_scale\")\n",
    "    parser.add_argument(\"--with_ema\", default=False, type=ast.literal_eval, help=\"training with ema\")\n",
    "    parser.add_argument(\"--save_every\", default=20, type=int, help=\"save every ___ epochs(default:20)\")\n",
    "    parser.add_argument('--train_url', default=\"./\", help='location of training outputs.')\n",
    "    parser.add_argument('--resize', type=int, default=224, help='Resize the image.')\n",
    "    parser.add_argument('--repeat_num', type=int, default=1, help='Number of repeat.')\n",
    "    parser.add_argument('--ckpt_save_dir', type=str, default=\"./ConvNeXt\", help='Location of training outputs.')\n",
    "    parser.add_argument(\"--ema_decay\", default=0.9999, type=float, help=\"ema decay\")\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    print(\"parameter is finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程使用的是ImageNet数据集，在mindspore中加载该数据集可使用两个接口：一是mindspore.dataset.GeneratorDataset，另一个是mindspore.dataset.ImageFolderDataset。考虑两者在加载数据集时网络的性能问题，本案例使用ImageFolderDataset接口对数据集进行加载。使该数据成为mindspore所能识别的格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将ImageNet数据集加载成mindspore所能识别的格式之后，使用RandomCropDecodeResize、RandomHorizontalFlip等诸多数据处理接口完成数据集的处理工作。\n",
    "\n",
    "本案例的create_dataset_imagenet函数中，包含了对训练集及验证集的处理操作，具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and eval dataset is finished.\n"
     ]
    }
   ],
   "source": [
    "import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as transforms\n",
    "import mindspore.dataset.vision.py_transforms as py_transforms\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision.utils import Inter\n",
    "\n",
    "from src.process_datasets.augment.auto_augment import pil_interp, rand_augment_transform\n",
    "from src.process_datasets.augment.mixup import Mixup\n",
    "from src.process_datasets.augment.random_erasing import RandomErasing\n",
    "\n",
    "\n",
    "def create_dataset_imagenet(dataset_dir, args, repeat_num=1, training=True):\n",
    "    \"\"\"\n",
    "    create a train or eval imagenet2012 dataset for TNT\n",
    "\n",
    "    Args:\n",
    "        dataset_dir(string): the path of dataset.\n",
    "        do_train(bool): whether dataset is used for train or eval.\n",
    "        repeat_num(int): the repeat times of dataset. Default: 1\n",
    "\n",
    "    Returns:\n",
    "        dataset\n",
    "    \"\"\"\n",
    "    # 使用ImageFolderDataset加载数据集\n",
    "    if training:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers,\n",
    "                                         shuffle=True)\n",
    "    else:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers,\n",
    "                                         shuffle=False)\n",
    "    image_size = 224\n",
    "    # define map operations\n",
    "    # BICUBIC: 3\n",
    "    if training:\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        aa_params = dict(\n",
    "            translate_const=int(image_size * 0.45),\n",
    "            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n",
    "        )\n",
    "        # 进行插值运算\n",
    "        interpolation = args.interpolation\n",
    "        # 数据增强\n",
    "        auto_augment = args.auto_augment\n",
    "        assert auto_augment.startswith('rand')\n",
    "        aa_params['interpolation'] = pil_interp(interpolation)\n",
    "\n",
    "        transform_img = [\n",
    "            transforms.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(3 / 4, 4 / 3),\n",
    "                                              interpolation=Inter.PILCUBIC),\n",
    "            transforms.RandomHorizontalFlip(prob=0.5),\n",
    "            py_transforms.ToPIL()\n",
    "        ]\n",
    "        transform_img += [rand_augment_transform(auto_augment, aa_params)]\n",
    "        transform_img += [\n",
    "            py_transforms.ToTensor(),\n",
    "            py_transforms.Normalize(mean=mean, std=std),\n",
    "            RandomErasing(args.re_prob, mode=args.re_mode, max_count=args.re_count)\n",
    "        ]\n",
    "    else:\n",
    "        mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "        std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "        # test transform complete\n",
    "        transform_img = [\n",
    "            transforms.Decode(),\n",
    "            transforms.Resize(int(256 / 224 * image_size), interpolation=Inter.PILCUBIC),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "            transforms.HWC2CHW()\n",
    "        ]\n",
    "\n",
    "    transform_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "    data_set = data_set.map(input_columns=\"image\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_img)\n",
    "    data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_label)\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.) and not training:\n",
    "        # if use mixup and not training(False), one hot val data label\n",
    "        one_hot = C.OneHot(num_classes=args.num_classes)\n",
    "        data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                operations=one_hot)\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(args.batch_size, drop_remainder=True,\n",
    "                              num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.) and training:\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mix_up, cutmix_alpha=args.cutmix, cutmix_minmax=None,\n",
    "            prob=args.mixup_prob, switch_prob=args.switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.label_smoothing, num_classes=args.num_classes)\n",
    "\n",
    "        data_set = data_set.map(operations=mixup_fn, input_columns=[\"image\", \"label\"],\n",
    "                                num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    # apply dataset repeat operation\n",
    "    data_set = data_set.repeat(repeat_num)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置数据加载及处理时所需的参数\n",
    "    args = parse_args()\n",
    "    train_dir = \"/home/ma-user/work/imagenet2012/train\"\n",
    "    val_dir = \"/home/ma-user/work/imagenet2012/val\"\n",
    "    # 加载并处理数据\n",
    "    dataset_train = create_dataset_imagenet(train_dir, args, 1, True)\n",
    "    dataset_eval = create_dataset_imagenet(val_dir, args, 1, False)\n",
    "    print(\"train and eval dataset is finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "ConvNeXt主干网络主要由ConvNextLayerNorm、Block模块组成，下面将主要介绍这两个网络模块。\n",
    "\n",
    "### ConvNextLayerNorm模块\n",
    "\n",
    "该模块与上述Various layer-wise Micro designs部分中的第三个微观改动相呼应。区别于其他网络，ConvNeXt网络不仅减少了正则化函数的使用，还将正则化函数由BatchNormal替换成LayerNormal。在该类中，当data_format为“channel_last”时，直接使用官方给出的mindspore.ops.LayerNorm进行正则化。反之，当data_format为“channel_first”时，先使用mindspore.ops.Transpose对数据进行通道的转换，使数据格式成为NHWC，再接着使用mindspore.ops.LayerNorm完成正则化任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下代码定义ConvNextLayerNorm类实现ConvNeXt LayerNorm结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops, nn\n",
    "\n",
    "\n",
    "class ConvNextLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"ConvNextLayerNorm\"\"\"\n",
    "    def __init__(self, normalized_shape, epsilon, norm_axis=-1):\n",
    "        super(ConvNextLayerNorm, self).__init__(normalized_shape=normalized_shape, epsilon=epsilon)\n",
    "        assert norm_axis in (-1, 1), \"ConvNextLayerNorm's norm_axis must be 1 or -1.\"\n",
    "        self.norm_axis = norm_axis\n",
    "\n",
    "    def construct(self, input_x):\n",
    "        if self.norm_axis == -1:\n",
    "            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n",
    "        else:\n",
    "            input_x = ops.Transpose()(input_x, (0, 2, 3, 1))\n",
    "            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n",
    "            y = ops.Transpose()(y, (0, 3, 1, 2))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block模块\n",
    "\n",
    "Block结构参照了在Transformer网络中的MLP模块及MobileNet V2中的Inverted Bottleneck模块，采用了“两头细，中间粗”的反瓶颈结构。同时，相较于经典的CNN网络使用$3\\times3$大小的卷积核和ReLU激活函数，该模块中使用了$7\\times7$大小的大卷积核以及GELU激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下代码定义Block类实现Block结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "from src.models.layers.drop_path import DropPath2D\n",
    "from src.models.layers.identity import Identity\n",
    "\n",
    "\n",
    "class Block(nn.Cell):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Dense -> GELU -> Dense; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, group=dim, has_bias=True)  # depthwise conv\n",
    "        self.norm = ConvNextLayerNorm((dim,), epsilon=1e-6)\n",
    "        self.pwconv1 = nn.Dense(dim, 4 * dim)  # pointwise/1x1 convs, implemented with Dense layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Dense(4 * dim, dim)\n",
    "        self.gamma = Parameter(Tensor(layer_scale_init_value * np.ones((dim)), dtype=mstype.float32),\n",
    "                               requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath2D(drop_path, 2) if drop_path > 0. else Identity()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Block construct\"\"\"\n",
    "        downsample = x\n",
    "        x = self.dwconv(x)\n",
    "        x = ops.Transpose()(x, (0, 2, 3, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = ops.Transpose()(x, (0, 3, 1, 2))\n",
    "        x = downsample + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DropPath2D层\n",
    "\n",
    "在上述Block结构中，应用了DropPath2D层，该层根据给定的概率drop_prob来随机选择网络上数值传递的路径进行drop，可以对整体的模型训练起到防止过拟合的作用，并且参数值根据keep_prob进行量化。\n",
    "\n",
    "如下代码给出DropPath2D的定义方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mindspore import nn, ops\n",
    "\n",
    "\n",
    "class DropPath2D(nn.Cell):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob, ndim):\n",
    "        super(DropPath2D, self).__init__()\n",
    "        self.drop = nn.Dropout(keep_prob=1 - drop_prob)\n",
    "        shape = (1,) + (1,) * (ndim + 1)\n",
    "        self.ndim = ndim\n",
    "        self.mask = Tensor(np.ones(shape), dtype=mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = ops.Tile()(self.mask, (x.shape[0],) + (1,) * (self.ndim + 1))\n",
    "        out = self.drop(mask)\n",
    "        out = out * x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建ConvNeXt主干网络\n",
    "\n",
    "ConvNeXt模型主干网络依次由四个downsample_layers层、以及四个stage层组成，其中，downsample_layers的第一层依次由nn.Conv2d、ConvNextLayerNorm构成，其余三层依次由ConvNextLayerNorm、nn.Conv2d构成。每个stage层由Block模块组成，其中每个stage层中利用depth参数控制Block个数，dims参数控制每部分的特征数目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在主干网络的代码中，值得一说的是init_weights函数。该函数可对网络中的nn.Dense以及nn.Conv2d的权重进行方差为0.02的TruncatedNormal初始化，对网络中Dense中的bias初始化为Zero。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common import initializer as weight_init\n",
    "\n",
    "def init_weights(self):\n",
    "    \"\"\"init_weights\"\"\"\n",
    "    for _, cell in self.cells_and_names():\n",
    "        if isinstance(cell, (nn.Dense, nn.Conv2d)):\n",
    "            cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n",
    "                                                         cell.weight.shape,\n",
    "                                                         cell.weight.dtype))\n",
    "            if isinstance(cell, nn.Dense) and cell.bias is not None:\n",
    "                cell.bias.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                           cell.bias.shape,\n",
    "                                                           cell.bias.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下代码定义ConvNeXt的主干网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore.common import initializer as weight_init\n",
    "\n",
    "\n",
    "class ConvNeXt(nn.Cell):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, num_classes, depths, dims, drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = nn.CellList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.SequentialCell(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, has_bias=True),\n",
    "            ConvNextLayerNorm((dims[0],), epsilon=1e-6, norm_axis=1)\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.SequentialCell(\n",
    "                ConvNextLayerNorm((dims[i],), epsilon=1e-6, norm_axis=1),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2, has_bias=True),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.CellList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates = [x for x in np.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            blocks = []\n",
    "            for j in range(depths[i]):\n",
    "                blocks.append(Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
    "                                    layer_scale_init_value=layer_scale_init_value))\n",
    "            stage = nn.SequentialCell(blocks)\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = ConvNextLayerNorm((dims[-1],), epsilon=1e-6)  # final norm layer\n",
    "        self.head = nn.Dense(dims[-1], num_classes)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.head.weight.set_data(self.head.weight * head_init_scale)\n",
    "        self.head.bias.set_data(self.head.bias * head_init_scale)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"init_weights\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, (nn.Dense, nn.Conv2d)):\n",
    "                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n",
    "                                                             cell.weight.shape,\n",
    "                                                             cell.weight.dtype))\n",
    "                if isinstance(cell, nn.Dense) and cell.bias is not None:\n",
    "                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                               cell.bias.shape,\n",
    "                                                               cell.bias.dtype))\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本案例传入ConvNeXt类中depths为[3, 3, 9, 3]，以及dims为[96, 192, 384, 768]，构成tiny规格的convnext_tiny模型，接下来将详细介绍convnext_tiny模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convnext_tiny模型\n",
    "\n",
    "convnext_tiny网络共有5个卷积结构，一个平均池化层，一个LayerNorm层，一个全连接层，以ImageNet数据集为例：\n",
    "\n",
    "+ **stem**：输入图片大小为$224\\times224$，输入channel为3.首先经过一个卷积核数量为96，卷积核大小为$4\\times4$，stride为4的卷积层，接着通过一个LayerNorm层。该层输出feature map大小为$56\\times56$，输出channel为96。\n",
    "+ **res2**：输入feature map大小为$56\\times56$，输入channel为96。经过堆叠3个$[d7\\times7，96；1\\times1，384；1\\times1，96]$结构的ConvNeXt Block。该层输出feature map大小为$56\\times56$，输出channel为96。\n",
    "+ **res3**：输入feature map大小为$56\\times56$，输入channel为96。经过依次堆叠3个$[d7\\times7，192；1\\times1，768；1\\times1，192]$结构的DownSample块及ConvNeXt Block块。该层输出feature map大小为$28\\times28$，输出channel为192。\n",
    "+ **res4**：输入feature map大小为$28\\times28$，输入channel为192。经过依次堆叠9个$[d7\\times7，384；1\\times1，1536；1\\times1，384]$结构的DownSample块及ConvNeXt Block块。该层输出feature map大小为$14\\times14$，输出channel为384。\n",
    "+ **res5**：输入feature map大小为$14\\times14$，输入channel为384。经过依次堆叠3个$[d7\\times7，768；1\\times1，3072；1\\times1，768]$结构的DownSample块及ConvNeXt Block块。该层输出feature map大小为$7\\times7$，输出channel为768。\n",
    "+ **average pool & LayerNorm & fc**：输入channel为768，输出channel为分类的类别数。\n",
    "\n",
    "如下示例代码实现convnext_tiny模型的构建，通过用调函数convnext_tiny即可构建convnext_tiny模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnext_tiny(args, **kwargs):\n",
    "    \"\"\"convnext_tiny\"\"\"\n",
    "    model = ConvNeXt(in_chans=3, num_classes=1000,\n",
    "                     depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                     drop_path_rate=0.1, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "本节调用convnext_tiny网络，然后定义AdamWeightDecay优化器和SoftTargetCrossEntropy损失函数，通过model.train接口对网络进行训练，其中，该网络采用混合精度的训练方法，使用支持单精度和半精度数据来提高网络的训练速度，同时保持单精度训练的精度。在训练的过程中将会打印训练的损失值，并保存评估精度最高的ckpt文件。下面主要介绍在训练过程中一些比较重要的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数分组\n",
    "\n",
    "在网络训练的过程中使用get_param_groups函数达到参数分组的目的。大多数其他经典网络在训练的过程中一般都是寻找所有可训练的参数，将其构建成一个列表，再将该列表传入优化器。但在本案例ConvNeXt网络的训练中，主要将参数分为两组，一组为不进行LR正则的参数，例如模块名称为bias的参数；另一组为进行LR正则的参数，例如模块名称为weight的参数。\n",
    "\n",
    "如下代码实现参数分组目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_groups(network):\n",
    "    \"\"\" get param groups \"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith(\".weight\"):\n",
    "            # Dense or Conv's weight using weight decay\n",
    "            decay_params.append(x)\n",
    "        else:\n",
    "            # all bias not using weight decay\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include LN\n",
    "            no_decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率\n",
    "\n",
    "在本案例中，学习率主要由两部分结合而成。如果当前epoch数小于学习率热身轮数时，此时的lr变换遵从wram_up的方法，否则，采用余弦下降的学习率，具体实现代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _warmup_lr(warmup_lr, base_lr, warmup_length, epoch):\n",
    "    \"\"\"Linear warmup\"\"\"\n",
    "    return epoch / warmup_length * (base_lr - warmup_lr) + warmup_lr\n",
    "\n",
    "\n",
    "def cosine_lr(args, batch_num):\n",
    "    \"\"\"Get cosine lr\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            e = epoch - args.warmup_length\n",
    "            es = args.epochs - args.warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * args.base_lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型前，需要先按照论文中给出的参数设置损失函数，优化器以及回调函数口。本案例为了方便展示，将epochs设置为10，sink_size设置为10，使其跑10轮，每轮跑10个step的数据，具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using amp_level O1\n",
      "=========================Using MixBatch=========================\n",
      "=> When using train_wrapper, using optimizer adamw\n",
      "=> Get LR from epoch: 0\n",
      "=> Start step: 0\n",
      "=> Total step: 1281000\n",
      "=> Accumulation step:1\n",
      "learning_rate 0.004\n",
      "=> Using DynamicLossScaleUpdateCell\n",
      "begin train\n",
      "epoch: 1 step: 10, loss is 7.037727355957031\n",
      "epoch time: 137322.846 ms, per step time: 13732.285 ms\n",
      "epoch: 2 step: 10, loss is 7.037711143493652\n",
      "epoch time: 29620.601 ms, per step time: 2962.060 ms\n",
      "epoch: 3 step: 10, loss is 7.011240005493164\n",
      "epoch time: 29268.201 ms, per step time: 2926.820 ms\n",
      "epoch: 4 step: 10, loss is 7.029295921325684\n",
      "epoch time: 29534.745 ms, per step time: 2953.474 ms\n",
      "epoch: 5 step: 10, loss is 6.974303245544434\n",
      "epoch time: 28968.114 ms, per step time: 2896.811 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mindspore import Model\n",
    "from mindspore import context\n",
    "from mindspore import nn\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "\n",
    "from src.models.loss import get_criterion, NetWithLoss, get_train_one_step\n",
    "from src.process_datasets.imagenet import ImageNet\n",
    "from src.models.convnext import convnext_tiny\n",
    "from src.utils.cell import cast_amp\n",
    "from src.utils.optimizer import get_optimizer\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    \"\"\"\"Train ConvNext model.\"\"\"\n",
    "    # 设置随机数及环境支持设备和模式\n",
    "    set_seed(args.seed)\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)\n",
    "   # 获取卡号和卡数\n",
    "    rank = 0\n",
    "    # 获取模型，并将模型cast成fp16\n",
    "    net = convnext_tiny(args)\n",
    "    cast_amp(net, args)\n",
    "    # 获取loss\n",
    "    criterion = get_criterion(args)\n",
    "    # 将loss和网络进行连接\n",
    "    net_with_loss = NetWithLoss(net, criterion)\n",
    "    # 获取训练数据集的大小\n",
    "    data = ImageNet(args, True)\n",
    "    batch_num = data.train_dataset.get_dataset_size()\n",
    "    # 获取Adamw优化器\n",
    "    optimizer = get_optimizer(args, net, batch_num)\n",
    "    # 对网络进行封装，使其具备train的条件\n",
    "    net_with_loss = get_train_one_step(args, net_with_loss, optimizer)\n",
    "    eval_network = nn.WithEvalCell(net, criterion, args.amp_level in [\"O2\", \"O3\", \"auto\"])\n",
    "    eval_indexes = [0, 1, 2]\n",
    "    model = Model(net_with_loss, metrics={\"acc\", \"loss\"},\n",
    "                  eval_network=eval_network,\n",
    "                  eval_indexes=eval_indexes)\n",
    "    # 设置保存多少个ckpt和时常计时器\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=data.train_dataset.get_dataset_size(),\n",
    "                                 keep_checkpoint_max=args.save_every)\n",
    "    time_cb = TimeMonitor(data_size=data.train_dataset.get_dataset_size())\n",
    "    # 设置ckpt的保存位置\n",
    "    ckpt_save_dir = \"./ckpt_\" + str(rank)\n",
    "    if args.run_modelarts:\n",
    "        ckpt_save_dir = \"/cache/ckpt_\" + str(rank)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=args.arch + str(rank), directory=ckpt_save_dir,\n",
    "                                 config=config_ck)\n",
    "    # 设置loss检测器\n",
    "    loss_cb = LossMonitor()\n",
    "    # 开始训练\n",
    "    print(\"begin train\")\n",
    "    model.train(5, data.train_dataset,\n",
    "                callbacks=[time_cb, ckpoint_cb, loss_cb],\n",
    "                dataset_sink_mode=True,\n",
    "                sink_size=10)\n",
    "    print(\"train success\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上述验证精度最高的模型对ImageNet测试数据集进行验证。在此过程中主要应用了Model、ImageNet、convnext_tiny、load_checkpoint、load_param_into_net等诸多接口。在评估的过程中，为方便后续云上运行及终端的调试工作，将load_checkpoint、load_param_into_net等接口封装在函数pretrained中，以下是函数pretrained的具体代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.convnext import convnext_tiny\n",
    "from src.utils.cell import cast_amp\n",
    "\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "\n",
    "def pretrained(args, model):\n",
    "    \"\"\"\"Load pretrained weights if args.pretrained is given\"\"\"\n",
    "    # 云上运行时，加载ckpt文件\n",
    "    if args.run_modelarts:\n",
    "        print('Syncing data.')\n",
    "        local_data_path = '/cache/weight'\n",
    "        name = args.pretrained.split('/')[-1]\n",
    "        path = f\"/\".join(args.pretrained.split(\"/\")[:-1])\n",
    "        sync_data(path, local_data_path, threads=128)\n",
    "        args.pretrained = os.path.join(local_data_path, name)\n",
    "        print(\"=> loading pretrained weights from '{}'\".format(args.pretrained))\n",
    "        param_dict = load_checkpoint(args.pretrained)\n",
    "        for key, value in param_dict.copy().items():\n",
    "            if 'head' in key:\n",
    "                if value.shape[0] != args.num_classes:\n",
    "                    print(f'==> removing {key} with shape {value.shape}')\n",
    "                    param_dict.pop(key)\n",
    "        # 将训练好的权重加载到网络模型中\n",
    "        load_param_into_net(model, param_dict)\n",
    "    # 终端使用时，加载ckpt文件\n",
    "    elif os.path.isfile(args.pretrained):\n",
    "        print(\"=> loading pretrained weights from '{}'\".format(args.pretrained))\n",
    "        param_dict = load_checkpoint(args.pretrained)\n",
    "        for key, value in param_dict.copy().items():\n",
    "            if 'head' in key:\n",
    "                if value.shape[0] != args.num_classes:\n",
    "                    print(f'==> removing {key} with shape {value.shape}')\n",
    "                    param_dict.pop(key)\n",
    "        load_param_into_net(model, param_dict)\n",
    "    else:\n",
    "        print(\"=> no pretrained weights found at '{}'\".format(args.pretrained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证流程大致可以描述为使用convnext_tiny接口定义网络结构，加载ImageNet数据集，并将[ckpt文件](https://download.mindspore.cn/vision/convnext/convnext_tiny0-300_533.ckpt)中的参数加载到定义好的网络结构中，随后设置损失函数，评价指标等等，最后对模型进行编译验证，其中本教程使用的评价标准为Top_1_Accuracy和Top_5_Accuracy。\n",
    "\n",
    "模型评估具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using amp_level O1\n",
      "=========================Using MixBatch=========================\n",
      "=> loading pretrained weights from '/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt'\n",
      "=> When using train_wrapper, using optimizer adamw\n",
      "=> Get LR from epoch: 0\n",
      "=> Start step: 0\n",
      "=> Total step: 49800\n",
      "=> Accumulation step:1\n",
      "learning_rate 0.004\n",
      "=> Using DynamicLossScaleUpdateCell\n",
      "=> begin eval\n",
      "=> eval results:{'Loss': 0.7911256961075657, 'Top1-Acc': 0.8230522088353414, 'Top5-Acc': 0.9593172690763052}\n",
      "=> eval success\n"
     ]
    }
   ],
   "source": [
    "from mindspore import Model\n",
    "from mindspore import context\n",
    "from mindspore import nn\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "from src.models.loss import get_criterion, NetWithLoss, get_train_one_step\n",
    "from src.models.convnext import convnext_tiny\n",
    "from src.utils.cell import cast_amp\n",
    "from src.utils.optimizer import get_optimizer\n",
    "from src.process_datasets.imagenet import ImageNet\n",
    "\n",
    "\n",
    "def pretrained(args, model):\n",
    "    \"\"\"\"Load pretrained weights if args.pretrained is given\"\"\"\n",
    "    print(\"=> loading pretrained weights from '{}'\".format(\"/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt\"))\n",
    "    param_dict = load_checkpoint(\"/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt\")\n",
    "    for key, value in param_dict.copy().items():\n",
    "        if 'head' in key:\n",
    "            if value.shape[0] != args.num_classes:\n",
    "                print(f'==> removing {key} with shape {value.shape}')\n",
    "                param_dict.pop(key)\n",
    "    load_param_into_net(model, param_dict)\n",
    "\n",
    "\n",
    "def convnext_eval(args):\n",
    "    \"\"\"\"Eval ConvNext model.\"\"\"\n",
    "    set_seed(args.seed)\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)\n",
    "    # 获取模型，并将模型cast成fp16\n",
    "    net = convnext_tiny(args)\n",
    "    cast_amp(net, args)\n",
    "    # 获取loss\n",
    "    criterion = get_criterion(args)\n",
    "    # 将获取到的loss与网络连接起来\n",
    "    net_with_loss = NetWithLoss(net, criterion)\n",
    "    # 将训练好的权重参数加载到网络模型中\n",
    "    pretrained(args, net)\n",
    "    # 加载验证数据集\n",
    "    data = ImageNet(args, training=False)\n",
    "    batch_num = data.val_dataset.get_dataset_size()\n",
    "    # 获取优化器\n",
    "    optimizer = get_optimizer(args, net, batch_num)\n",
    "    # 将网络封装为可训练的模式\n",
    "    net_with_loss = get_train_one_step(args, net_with_loss, optimizer)\n",
    "    eval_network = nn.WithEvalCell(net, criterion, args.amp_level in [\"O2\", \"O3\", \"auto\"])\n",
    "    eval_indexes = [0, 1, 2]\n",
    "    # 加入评价指标Top1-Acc和Top5-Acc\n",
    "    eval_metrics = {'Loss': nn.Loss(),\n",
    "                    'Top1-Acc': nn.Top1CategoricalAccuracy(),\n",
    "                    'Top5-Acc': nn.Top5CategoricalAccuracy()}\n",
    "    model = Model(net_with_loss, metrics=eval_metrics,\n",
    "                  eval_network=eval_network,\n",
    "                  eval_indexes=eval_indexes)\n",
    "    # 开始验证\n",
    "    print(f\"=> begin eval\")\n",
    "    results = model.eval(data.val_dataset)\n",
    "    print(f\"=> eval results:{results}\")\n",
    "    print(f\"=> eval success\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convnext_eval(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "{'Top1-Acc': 0.8230522088353414, 'Top5-Acc': 0.9593172690763052}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型推理过程较为简单，首先需要使用ImageNet数据集接口读取要推理的图片，接着对读取到的图片进行Decode、Resize、CenterCrop等操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在处理完推理图片之后，将训练好的参数加载到网络中，紧接着通过Model.predict方法对图片进行推理即可，具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using amp_level O1\n",
      "=========================Using MixBatch=========================\n",
      "=> loading pretrained weights from '/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt'\n",
      "i is : 0\n",
      "predict is finished.\n",
      "{394: 'sturgeon'}\n",
      "i is : 1\n",
      "predict is finished.\n",
      "{394: 'sturgeon'}\n",
      "i is : 2\n",
      "predict is finished.\n",
      "{394: 'sturgeon'}\n",
      "i is : 3\n",
      "predict is finished.\n",
      "{394: 'sturgeon'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import pathlib\n",
    "from typing import Dict, Optional\n",
    "from PIL import Image\n",
    "from scipy import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import mindspore as ms\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.train import Model\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.common import set_seed\n",
    "from mindspore import context\n",
    "import mindspore.dataset.vision.c_transforms as transforms\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision.utils import Inter\n",
    "\n",
    "from src.utils.cell import cast_amp\n",
    "from src.models.convnext import convnext_tiny\n",
    "\n",
    "\n",
    "def pretrained(args, model):\n",
    "    \"\"\"\"Load pretrained weights if args.pretrained is given\"\"\"\n",
    "    print(\"=> loading pretrained weights from '{}'\".format(\"/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt\"))\n",
    "    param_dict = load_checkpoint(\"/home/ma-user/work/check/src/train_parallel0/ckpt_0/convnext_tiny0-300_533.ckpt\")\n",
    "    for key, value in param_dict.copy().items():\n",
    "        if 'head' in key:\n",
    "            if value.shape[0] != args.num_classes:\n",
    "                print(f'==> removing {key} with shape {value.shape}')\n",
    "                param_dict.pop(key)\n",
    "    load_param_into_net(model, param_dict)\n",
    "\n",
    "\n",
    "def infer(args):\n",
    "    \"\"\"\"infer ConvNext model.\"\"\"\n",
    "    set_seed(args.seed)\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)\n",
    "    # get model\n",
    "    net = convnext_tiny(args)\n",
    "    cast_amp(net, args)\n",
    "    criterion = get_criterion(args)\n",
    "    NetWithLoss(net, criterion)\n",
    "    pretrained(args, net)\n",
    "    # Read data for inference\n",
    "    dataset_infer = ds.ImageFolderDataset(os.path.join(args.data_url, \"infer\"), shuffle=True)\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "    # test transform complete\n",
    "    transform_img = [\n",
    "        transforms.Decode(),\n",
    "        transforms.Resize(int(256 / 224 * 224), interpolation=Inter.PILCUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        transforms.HWC2CHW()\n",
    "        ]\n",
    "    transform_label = C.TypeCast(mstype.int32)\n",
    "    dataset_infer = dataset_infer.map(input_columns=\"image\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                      operations=transform_img)\n",
    "    dataset_infer = dataset_infer.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                      operations=transform_label)\n",
    "    one_hot = C.OneHot(num_classes=args.num_classes)\n",
    "    dataset_infer = dataset_infer.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                      operations=one_hot)\n",
    "    # apply batch operations\n",
    "    dataset_infer = dataset_infer.batch(1, drop_remainder=True,\n",
    "                                        num_parallel_workers=args.num_parallel_workers)\n",
    "    model = Model(net)\n",
    "    for i, image in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "        print('i is :', i)\n",
    "        image = image[\"image\"]\n",
    "        image = ms.Tensor(image)\n",
    "        prob = model.predict(image)\n",
    "        print(\"predict is finished.\")\n",
    "        label = np.argmax(prob.asnumpy(), axis=1)\n",
    "        mapping = index2label(args)\n",
    "        output = {int(label): mapping[int(label)]}\n",
    "        print(output)\n",
    "        show_result(img=\"/home/ma-user/work/imagenet2012/infer/n01440764/ILSVRC2012_test_00000293.jpg\",\n",
    "                    result=output,\n",
    "                    out_file=\"/home/ma-user/work/imagenet2012/infer/ILSVRC2012_test_00000.JPEG\")\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    \"\"\"dedine enum color.\"\"\"\n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (255, 0, 0)\n",
    "    cyan = (255, 255, 0)\n",
    "    yellow = (0, 255, 255)\n",
    "    magenta = (255, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "\n",
    "\n",
    "def check_file_exist(file_name: str):\n",
    "    \"\"\"check_file_exist.\"\"\"\n",
    "    if not os.path.isfile(file_name):\n",
    "        raise FileNotFoundError(f\"File `{file_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def color_val(color):\n",
    "    \"\"\"color_val.\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        return Color[color].value\n",
    "    if isinstance(color, Color):\n",
    "        return color.value\n",
    "    if isinstance(color, tuple):\n",
    "        assert len(color) == 3\n",
    "        for channel in color:\n",
    "            assert 0 <= channel <= 255\n",
    "        return color\n",
    "    if isinstance(color, int):\n",
    "        assert 0 <= color <= 255\n",
    "        return color, color, color\n",
    "    if isinstance(color, np.ndarray):\n",
    "        assert color.ndim == 1 and color.size == 3\n",
    "        assert np.all((color >= 0) & (color <= 255))\n",
    "        color = color.astype(np.uint8)\n",
    "        return tuple(color)\n",
    "    raise TypeError(f'Invalid type for color: {type(color)}')\n",
    "\n",
    "\n",
    "def imread(image, mode=None):\n",
    "    \"\"\"imread.\"\"\"\n",
    "    if isinstance(image, pathlib.Path):\n",
    "        image = str(image)\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(image, str):\n",
    "        check_file_exist(image)\n",
    "        image = Image.open(image)\n",
    "        if mode:\n",
    "            image = np.array(image.convert(mode))\n",
    "    else:\n",
    "        raise TypeError(\"Image must be a `ndarray`, `str` or Path object.\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def imwrite(image, image_path, auto_mkdir=True):\n",
    "    \"\"\"imwrite.\"\"\"\n",
    "    if auto_mkdir:\n",
    "        dir_name = os.path.abspath(os.path.dirname(image_path))\n",
    "        if dir_name != '':\n",
    "            dir_name = os.path.expanduser(dir_name)\n",
    "            os.makedirs(dir_name, mode=777, exist_ok=True)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "def imshow(img, win_name='', wait_time=0):\n",
    "    \"\"\"imshow\"\"\"\n",
    "    cv2.imshow(win_name, imread(img))\n",
    "    if wait_time == 0:  # prevent from hanging if windows was closed\n",
    "        while True:\n",
    "            ret = cv2.waitKey(1)\n",
    "\n",
    "            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1\n",
    "            # if user closed window or if some key pressed\n",
    "            if closed or ret != -1:\n",
    "                break\n",
    "    else:\n",
    "        ret = cv2.waitKey(wait_time)\n",
    "\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, float],\n",
    "                text_color: str = 'green',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    \"\"\"Mark the prediction results on the picture.\"\"\"\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "    x, y = 0, row_width\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        if isinstance(v, float):\n",
    "            v = f'{v:.2f}'\n",
    "        label_text = f'{k}: {v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color)\n",
    "        y += row_width\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n",
    "\n",
    "\n",
    "def index2label(args):\n",
    "    \"\"\"Dictionary output for image numbers and categories of the ImageNet dataset.\"\"\"\n",
    "    metafile = os.path.join(args.data_url, \"ILSVRC2012_devkit_t12/data/meta.mat\")\n",
    "    meta = io.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "\n",
    "    nums_children = list(zip(*meta))[4]\n",
    "    meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "\n",
    "    _, wnids, classes = list(zip(*meta))[:3]\n",
    "    clssname = [tuple(clss.split(', ')) for clss in classes]\n",
    "    wnid2class = {wnid: clss for wnid, clss in zip(wnids, clssname)}\n",
    "    wind2class_name = sorted(wnid2class.items(), key=lambda x: x[0])\n",
    "\n",
    "    mapping = {}\n",
    "    for index, (_, class_name) in enumerate(wind2class_name):\n",
    "        mapping[index] = class_name[0]\n",
    "    return mapping\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理后的图片如下图所示：\n",
    "<div align=center><img src=\"./images/convnext_infer.jpg\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程实现了一个ConvNeXt模型在ImageNet数据集上进行训练、验证和推理的过程。其中对ConvNeXt模型结构和原理做了简单介绍。\n",
    "\n",
    "> 如果要详细了解ConvNeXt模型的工作原理，建议对源码进行深层次的阅读，可以参考以下链接:\n",
    "> https://gitee.com/mindspore/course/tree/master/application_example/convnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用\n",
    "\n",
    "[1] Liu Z ,  Mao H ,  Wu C Y , et al. A ConvNet for the 2020s[J]. arXiv e-prints, 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
